{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modelling\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_new_csv(original_csv_path, new_csv_path, data_root, max_duration=6):\n",
    "    \"\"\"Filters audio files in a CSV based on duration and saves to a new CSV.\n",
    "\n",
    "    Args:\n",
    "        original_csv_path (str): Path to the original CSV file.\n",
    "        new_csv_path (str): Path to save the new filtered CSV file.\n",
    "        data_root (str): Root directory of the audio files.\n",
    "        max_duration (int, optional): Maximum duration in seconds. Defaults to 6.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(original_csv_path)\n",
    "    df['duration'] = df['filename'].apply(lambda filename: torchaudio.info(os.path.join(data_root, filename)).num_frames / torchaudio.info(os.path.join(data_root, filename)).sample_rate)\n",
    "    filtered_df = df[df['duration'] <= max_duration]\n",
    "    filtered_df = filtered_df.drop(columns=['duration'])\n",
    "    filtered_df.to_csv(new_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "# filter_to_new_csv(\n",
    "#     original_csv_path=\"../data/common_voice/cv-valid-train.csv\",\n",
    "#     new_csv_path=\"../data/common_voice/cv-valid-train_filtered.csv\",\n",
    "#     data_root=\"../data/resampled_audio\"\n",
    "# )\n",
    "\n",
    "# filter_to_new_csv(\n",
    "#     original_csv_path=\"../data/common_voice/cv-valid-test.csv\",\n",
    "#     new_csv_path=\"../data/common_voice/cv-valid-test_filtered.csv\",\n",
    "#     data_root=\"../data/resampled_audio\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.freeze_feature_extractor()  # feature extractor\n",
    "\n",
    "# Optionally freeze first few transformer layers:\n",
    "for i in range(6):  # freeze first 6 out of 12 (for base model)\n",
    "    for param in model.wav2vec2.encoder.layers[i].parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor, data_root=\"\", split=\"train\", val_split=0.3):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # Capitalize the 'text' column\n",
    "        self.df['text'] = self.df['text'].str.upper()\n",
    "        self.processor = processor\n",
    "        self.data_root = data_root\n",
    "\n",
    "        if split == \"train\":\n",
    "            # Split into train and validation sets\n",
    "            train_df, val_df = train_test_split(self.df, test_size=val_split, random_state=42)  # Use random_state for reproducibility\n",
    "            self.df = train_df.reset_index(drop=True)  # Reset index for the training data\n",
    "        elif split == \"val\":\n",
    "            # Split into train and validation sets\n",
    "            train_df, val_df = train_test_split(self.df, test_size=val_split, random_state=42)  # Use random_state for reproducibility\n",
    "            self.df = val_df.reset_index(drop=True) # Reset index for the validation data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_audio_path = self.df.loc[idx, \"filename\"]\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        audio_path = os.path.join(self.data_root, rel_audio_path)\n",
    "\n",
    "        audio, sr = torchaudio.load(audio_path)\n",
    "        input_values = self.processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_values\n",
    "        input_values = input_values[0]\n",
    "\n",
    "        labels = self.processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values.squeeze(0),\n",
    "            \"labels\": labels.squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your custom datasets\n",
    "train_dataset = AudioTextDataset(\n",
    "    csv_path=\"../data/common_voice/cv-valid-train_filtered.csv\",\n",
    "    processor=processor,\n",
    "    data_root=\"../data/resampled_audio\",\n",
    "    split=\"train\"  # Specify split=\"train\" for training data\n",
    ")\n",
    "\n",
    "val_dataset = AudioTextDataset(\n",
    "    csv_path=\"../data/common_voice/cv-valid-train_filtered.csv\",  # Same CSV as training\n",
    "    processor=processor,\n",
    "    data_root=\"../data/resampled_audio\",\n",
    "    split=\"val\"  # Specify split=\"val\" for validation data\n",
    ")\n",
    "\n",
    "test_dataset = AudioTextDataset(\n",
    "    csv_path=\"../data/common_voice/cv-valid-test_filtered.csv\",\n",
    "    processor=processor,\n",
    "    data_root=\"../data/resampled_audio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    input_values = [item[\"input_values\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Wrap each input value in a dict\n",
    "    input_values = [{\"input_values\": v} for v in input_values]\n",
    "    labels = [{\"input_ids\": l} for l in labels]\n",
    "\n",
    "    # Pad input values\n",
    "    input_values_padded = processor.pad(\n",
    "        input_values,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values\n",
    "\n",
    "    # Pad labels\n",
    "    labels_padded = processor.pad(\n",
    "        labels,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    # Replace pad tokens with -100\n",
    "    labels_padded[labels_padded == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_values\": input_values_padded,\n",
    "        \"labels\": labels_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/HTX/wav2vec2-large-960h-cv\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_steps=2000,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=500,\n",
    "    #gradient_accumulation_steps = 8,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,  # Mixed precision for faster training on GPUs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/5xp91zzx3_ncbb92pthw9_k40000gn/T/ipykernel_54291/1636119560.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandyscrub/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_ctc_loss' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 2236df1770800ffea5697b11b0bb0d910b2e59e1. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/transformers/trainer.py:2556\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2549\u001b[39m context = (\n\u001b[32m   2550\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2553\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2554\u001b[39m )\n\u001b[32m   2555\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2556\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2559\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2561\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2562\u001b[39m ):\n\u001b[32m   2563\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2564\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/transformers/trainer.py:3718\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3715\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3717\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3718\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3720\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3722\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3723\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3724\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/transformers/trainer.py:3783\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3781\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3782\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3783\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3784\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3785\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2258\u001b[39m, in \u001b[36mWav2Vec2ForCTC.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[39m\n\u001b[32m   2255\u001b[39m     log_probs = nn.functional.log_softmax(logits, dim=-\u001b[32m1\u001b[39m, dtype=torch.float32).transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m   2257\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.backends.cudnn.flags(enabled=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2258\u001b[39m         loss = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflattened_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2263\u001b[39m \u001b[43m            \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2264\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctc_loss_reduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2265\u001b[39m \u001b[43m            \u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctc_zero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   2269\u001b[39m     output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NUS/Y4S2/HTX/xData/new2/technical_test/venv/lib/python3.13/site-packages/torch/nn/functional.py:3079\u001b[39m, in \u001b[36mctc_loss\u001b[39m\u001b[34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[39m\n\u001b[32m   3067\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[32m   3068\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   3069\u001b[39m         ctc_loss,\n\u001b[32m   3070\u001b[39m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[32m   (...)\u001b[39m\u001b[32m   3077\u001b[39m         zero_infinity=zero_infinity,\n\u001b[32m   3078\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: The operator 'aten::_ctc_loss' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 2236df1770800ffea5697b11b0bb0d910b2e59e1. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = Wav2Vec2ForCTC.from_pretrained(\"wav2vec2-large-960h-cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-valid-test/sample-000000.mp3</td>\n",
       "      <td>without the dataset the article is useless</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-valid-test/sample-000001.mp3</td>\n",
       "      <td>i've got to go to him</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv-valid-test/sample-000002.mp3</td>\n",
       "      <td>and you know it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-valid-test/sample-000003.mp3</td>\n",
       "      <td>down below in the darkness were hundreds of pe...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv-valid-test/sample-000004.mp3</td>\n",
       "      <td>hold your nose to keep the smell from disablin...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>cv-valid-test/sample-003990.mp3</td>\n",
       "      <td>the old man opened his cape and the boy was st...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>cv-valid-test/sample-003991.mp3</td>\n",
       "      <td>in alchemy it's called the soul of the world</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>cv-valid-test/sample-003992.mp3</td>\n",
       "      <td>at that point in their lives everything is cle...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>cv-valid-test/sample-003993.mp3</td>\n",
       "      <td>he told them all to be seated</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>cv-valid-test/sample-003994.mp3</td>\n",
       "      <td>the restaurant was quite expensive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3995 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             filename  \\\n",
       "0     cv-valid-test/sample-000000.mp3   \n",
       "1     cv-valid-test/sample-000001.mp3   \n",
       "2     cv-valid-test/sample-000002.mp3   \n",
       "3     cv-valid-test/sample-000003.mp3   \n",
       "4     cv-valid-test/sample-000004.mp3   \n",
       "...                               ...   \n",
       "3990  cv-valid-test/sample-003990.mp3   \n",
       "3991  cv-valid-test/sample-003991.mp3   \n",
       "3992  cv-valid-test/sample-003992.mp3   \n",
       "3993  cv-valid-test/sample-003993.mp3   \n",
       "3994  cv-valid-test/sample-003994.mp3   \n",
       "\n",
       "                                                   text  up_votes  down_votes  \\\n",
       "0            without the dataset the article is useless         1           0   \n",
       "1                                 i've got to go to him         1           0   \n",
       "2                                       and you know it         1           0   \n",
       "3     down below in the darkness were hundreds of pe...         4           0   \n",
       "4     hold your nose to keep the smell from disablin...         2           0   \n",
       "...                                                 ...       ...         ...   \n",
       "3990  the old man opened his cape and the boy was st...         1           0   \n",
       "3991       in alchemy it's called the soul of the world         2           1   \n",
       "3992  at that point in their lives everything is cle...         3           0   \n",
       "3993                      he told them all to be seated         3           0   \n",
       "3994                 the restaurant was quite expensive         2           0   \n",
       "\n",
       "           age gender accent  duration  \\\n",
       "0          NaN    NaN    NaN       NaN   \n",
       "1     twenties   male    NaN       NaN   \n",
       "2          NaN    NaN    NaN       NaN   \n",
       "3     twenties   male     us       NaN   \n",
       "4          NaN    NaN    NaN       NaN   \n",
       "...        ...    ...    ...       ...   \n",
       "3990       NaN    NaN    NaN       NaN   \n",
       "3991       NaN    NaN    NaN       NaN   \n",
       "3992       NaN    NaN    NaN       NaN   \n",
       "3993       NaN    NaN    NaN       NaN   \n",
       "3994       NaN    NaN    NaN       NaN   \n",
       "\n",
       "                                              file_path  \n",
       "0     ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "1     ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "2     ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "3     ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "4     ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "...                                                 ...  \n",
       "3990  ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "3991  ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "3992  ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "3993  ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "3994  ../data/resampled_audio/cv-valid-test/sample-0...  \n",
       "\n",
       "[3995 rows x 9 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import \n",
    "test_df = pd.read_csv(\"../data/common_voice/cv-valid-test.csv\")\n",
    "\n",
    "# Create filepath col to audiofiles \n",
    "test_df['file_path'] = test_df['filename'].apply(lambda x: os.path.join(\"../data/resampled_audio/\", x))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>WITHOUT THE DATASET THE ARTICLE IS USELESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>I'VE GOT TO GO TO HIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>AND YOU KNOW IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>DOWN BELOW IN THE DARKNESS WERE HUNDREDS OF PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>HOLD YOUR NOSE TO KEEP THE SMELL FROM DISABLIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>THE OLD MAN OPENED HIS CAPE AND THE BOY WAS ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>IN ALCHEMY IT'S CALLED THE SOUL OF THE WORLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>AT THAT POINT IN THEIR LIVES EVERYTHING IS CLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>HE TOLD THEM ALL TO BE SEATED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>THE RESTAURANT WAS QUITE EXPENSIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3995 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_path  \\\n",
       "0     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "1     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "2     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "4     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "...                                                 ...   \n",
       "3990  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3991  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3992  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3993  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3994  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "\n",
       "                                                   text  \n",
       "0            WITHOUT THE DATASET THE ARTICLE IS USELESS  \n",
       "1                                 I'VE GOT TO GO TO HIM  \n",
       "2                                       AND YOU KNOW IT  \n",
       "3     DOWN BELOW IN THE DARKNESS WERE HUNDREDS OF PE...  \n",
       "4     HOLD YOUR NOSE TO KEEP THE SMELL FROM DISABLIN...  \n",
       "...                                                 ...  \n",
       "3990  THE OLD MAN OPENED HIS CAPE AND THE BOY WAS ST...  \n",
       "3991       IN ALCHEMY IT'S CALLED THE SOUL OF THE WORLD  \n",
       "3992  AT THAT POINT IN THEIR LIVES EVERYTHING IS CLE...  \n",
       "3993                      HE TOLD THEM ALL TO BE SEATED  \n",
       "3994                 THE RESTAURANT WAS QUITE EXPENSIVE  \n",
       "\n",
       "[3995 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_subset = test_df[['file_path', 'text']].copy()\n",
    "test_df_subset['text'] = test_df_subset['text'].str.upper()\n",
    "test_df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, processor, audio_path):\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    input_values = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_values\n",
    "    input_values = input_values[0]\n",
    "\n",
    "    # retrieve logits & take argmax\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    transcription = processor.decode(predicted_ids[0]) \n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WITHOUT A DATASET THE ARTICLE IS USELESS'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(finetuned_model, processor, \"../data/resampled_audio/cv-valid-test/sample-000000.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>WITHOUT THE DATASET THE ARTICLE IS USELESS</td>\n",
       "      <td>WITHOUT A DATASET THE ARTICLE IS USELESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>I'VE GOT TO GO TO HIM</td>\n",
       "      <td>I'VE GOT TO GO TO HIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>AND YOU KNOW IT</td>\n",
       "      <td>AND YOU KNOW IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>DOWN BELOW IN THE DARKNESS WERE HUNDREDS OF PE...</td>\n",
       "      <td>DOWN BELOW IN THE DARKNESS WERE HUNDREDS OF PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>HOLD YOUR NOSE TO KEEP THE SMELL FROM DISABLIN...</td>\n",
       "      <td>HOLD YOUR NOSE TO KEEP THE SMELL FROM DISABLIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>THE OLD MAN OPENED HIS CAPE AND THE BOY WAS ST...</td>\n",
       "      <td>THE OLD MAN OPENED HIS CAPE AND THE BOY WAS ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>IN ALCHEMY IT'S CALLED THE SOUL OF THE WORLD</td>\n",
       "      <td>AN ALCHEMY ITS CALLED THE SOUL OF THE WORLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>AT THAT POINT IN THEIR LIVES EVERYTHING IS CLE...</td>\n",
       "      <td>AT THAT POINT IN THEIR LIVES EVERYTHING IS CLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>HE TOLD THEM ALL TO BE SEATED</td>\n",
       "      <td>HE TOLD THEM ALL TO BE SEATED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>../data/resampled_audio/cv-valid-test/sample-0...</td>\n",
       "      <td>THE RESTAURANT WAS QUITE EXPENSIVE</td>\n",
       "      <td>THE RESTAURANT WAS QUITE EXPENSILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3995 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_path  \\\n",
       "0     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "1     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "2     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "4     ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "...                                                 ...   \n",
       "3990  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3991  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3992  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3993  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "3994  ../data/resampled_audio/cv-valid-test/sample-0...   \n",
       "\n",
       "                                                   text  \\\n",
       "0            WITHOUT THE DATASET THE ARTICLE IS USELESS   \n",
       "1                                 I'VE GOT TO GO TO HIM   \n",
       "2                                       AND YOU KNOW IT   \n",
       "3     DOWN BELOW IN THE DARKNESS WERE HUNDREDS OF PE...   \n",
       "4     HOLD YOUR NOSE TO KEEP THE SMELL FROM DISABLIN...   \n",
       "...                                                 ...   \n",
       "3990  THE OLD MAN OPENED HIS CAPE AND THE BOY WAS ST...   \n",
       "3991       IN ALCHEMY IT'S CALLED THE SOUL OF THE WORLD   \n",
       "3992  AT THAT POINT IN THEIR LIVES EVERYTHING IS CLE...   \n",
       "3993                      HE TOLD THEM ALL TO BE SEATED   \n",
       "3994                 THE RESTAURANT WAS QUITE EXPENSIVE   \n",
       "\n",
       "                                         predicted_text  \n",
       "0              WITHOUT A DATASET THE ARTICLE IS USELESS  \n",
       "1                                 I'VE GOT TO GO TO HIM  \n",
       "2                                       AND YOU KNOW IT  \n",
       "3     DOWN BELOW IN THE DARKNESS WERE HUNDREDS OF PE...  \n",
       "4     HOLD YOUR NOSE TO KEEP THE SMELL FROM DISABLIN...  \n",
       "...                                                 ...  \n",
       "3990  THE OLD MAN OPENED HIS CAPE AND THE BOY WAS ST...  \n",
       "3991        AN ALCHEMY ITS CALLED THE SOUL OF THE WORLD  \n",
       "3992  AT THAT POINT IN THEIR LIVES EVERYTHING IS CLE...  \n",
       "3993                      HE TOLD THEM ALL TO BE SEATED  \n",
       "3994                 THE RESTAURANT WAS QUITE EXPENSILE  \n",
       "\n",
       "[3995 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_subset['predicted_text'] = test_df_subset['file_path'].apply(lambda x: get_prediction(finetuned_model, processor, x))\n",
    "test_df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.49k/4.49k [00:00<00:00, 6.18MB/s]\n",
      "Downloading builder script: 100%|██████████| 5.60k/5.60k [00:00<00:00, 32.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the Finetuned Model's Overall Word Error Rate (WER): 0.07458308005391548\n",
      "this is the Finetuned Model's Overall Character Error Rate (CER): 0.03135823657737203\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "wer = wer_metric.compute(references=test_df_subset['text'].tolist(), predictions=test_df_subset['predicted_text'].tolist())\n",
    "cer = cer_metric.compute(references=test_df_subset['text'].tolist(), predictions=test_df_subset['predicted_text'].tolist())\n",
    "\n",
    "print(f\"this is the Finetuned Model's Overall Word Error Rate (WER): {wer}\")\n",
    "print(f\"this is the Finetuned Model's Overall Character Error Rate (CER): {cer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
